<b> Partitioner controls the partitioning of what data?</b><br><ul><li>final keys</li><li>final values</li><li>intermediate keys</li><li>intermediate values</li></ul>,intermediate keys
"<b> SQL Windowing functions are implemented in Hive using which keywords?</b><br><ul><li>UNION DISTINCT, RANK</li><li>OVER, RANK</li><li>OVER, EXCEPT</li><li>UNION DISTINCT, RANK</li></ul>","OVER, RANK"
"<b> Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?</b><br><ul><li>Add a partitioned shuffle to the Map job.</li><li>Add a partitioned shuffle to the Reduce job.</li><li>Break the Reduce job into multiple, chained Reduce jobs.</li><li>Break the Reduce job into multiple, chained Map jobs.</li></ul>",Add a partitioned shuffle to the Reduce job.
"<b> Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?</b><br><ul><li>encrypted HTTP</li><li>unsigned HTTP</li><li>compressed HTTP</li><li>signed HTTP</li></ul>",signed HTTP
<b> MapReduce jobs can be written in which language?</b><br><ul><li>Java or Python</li><li>SQL only</li><li>SQL or Java</li><li>Python or SQL</li></ul>,Java or Python
"<b> To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?</b><br><ul><li>Reducer</li><li>Combiner</li><li>Mapper</li><li>Counter</li></ul>",Combiner
"<b> To verify job status, look for the value `___` in the `___`.</b><br><ul><li>SUCCEEDED; syslog</li><li>SUCCEEDED; stdout</li><li>DONE; syslog</li><li>DONE; stdout</li></ul>",SUCCEEDED; stdout
"<b> Which line of code implements a Reducer method in MapReduce </b><br><ul><li>public void reduce(Text key, Iterator<IntWritable> values, Context context){…}</li><li>public static void reduce(Text key, IntWritable[] values, Context context){…}</li><li>public static void reduce(Text key, Iterator<IntWritable> values, Context context){…}</li><li>public void reduce(Text key, IntWritable[] values, Context context){…}</li></ul>","public void reduce(Text key, Iterator<IntWritable> values, Context context){…}"
"<b> To get the total number of mapped input records in a map job task, you should review the value of which counter?</b><br><ul><li>FileInputFormatCounter</li><li>FileSystemCounter</li><li>JobCounter</li><li>TaskCounter (NOT SURE)</li></ul>",TaskCounter (NOT SURE)
"<b> Hadoop Core supports which CAP capabilities?</b><br><ul><li>A, P</li><li>C, A</li><li>C, P</li><li>C, A, P</li></ul>","A, P"
"<b> What are the primary phases of a Reducer?</b><br><ul><li>combine, map, and reduce</li><li>shuffle, sort, and reduce</li><li>reduce, sort, and combine</li><li>map, sort, and combine</li></ul>","shuffle, sort, and reduce"
"<b> To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the `___` service, which is `___`.</b><br><ul><li>Oozie; open source</li><li>Oozie; commercial software</li><li>Zookeeper; commercial software</li><li>Zookeeper; open source</li></ul>",Zookeeper; open source
"<b> For high availability, use multiple nodes of which type?</b><br><ul><li>data</li><li>name</li><li>memory</li><li>worker</li></ul>",name
<b> DataNode supports which type of drives?</b><br><ul><li>hot swappable</li><li>cold swappable</li><li>warm swappable</li><li>non-swappable</li></ul>,hot swappable
<b> Which method is used to implement Spark jobs?</b><br><ul><li>on disk of all workers</li><li>on disk of the master node</li><li>in memory of the master node</li><li>in memory of all workers</li></ul>,in memory of all workers
"<b> In a MapReduce job, where does the map() function run?</b><br><ul><li>on the reducer nodes of the cluster</li><li>on the data nodes of the cluster (NOT SURE)</li><li>on the master node of the cluster</li><li>on every node of the cluster</li></ul>",on the data nodes of the cluster (NOT SURE)
"<b> To reference a master file for lookups during Mapping, what type of cache should be used?</b><br><ul><li>distributed cache</li><li>local cache</li><li>partitioned cache</li><li>cluster cache</li></ul>",distributed cache
<b> Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?</b><br><ul><li>cache inputs</li><li>reducer inputs</li><li>intermediate values</li><li>map inputs</li></ul>,map inputs
<b> Which command imports data to Hadoop from a MySQL database?</b><br><ul><li>spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --warehouse-dir user/hue/oozie/deployments/spark</li><li>sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --warehouse-dir user/hue/oozie/deployments/sqoop</li><li>sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop</li><li>spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --password spark --warehouse-dir user/hue/oozie/deployments/spark</li></ul>,sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop
<b> In what form is Reducer output presented?</b><br><ul><li>compressed (NOT SURE)</li><li>sorted</li><li>not sorted</li><li>encrypted</li></ul>,compressed (NOT SURE)
<b> Which library should be used to unit test MapReduce code?</b><br><ul><li>JUnit</li><li>XUnit</li><li>MRUnit</li><li>HadoopUnit</li></ul>,MRUnit
"<b> If you started the NameNode, then which kind of user must you be?</b><br><ul><li>hadoop-user</li><li>super-user</li><li>node-user</li><li>admin-user</li></ul>",super-user
<b> State \_ between the JVMs in a MapReduce job</b><br><ul><li>can be configured to be shared</li><li>is partially shared</li><li>is shared</li><li>is not shared (https://www.lynda.com/Hadoop-tutorials/Understanding-Java-virtual-machines-JVMs/191942/369545-4.html)</li></ul>,is not shared (https://www.lynda.com/Hadoop-tutorials/Understanding-Java-virtual-machines-JVMs/191942/369545-4.html)
"<b> To create a MapReduce job, what should be coded first?</b><br><ul><li>a static job() method</li><li>a Job class and instance (NOT SURE)</li><li>a job() method</li><li>a static Job class</li></ul>",a Job class and instance (NOT SURE)
<b> To connect Hadoop to AWS S</b><br><ul><li>S3A</li><li>S3N</li><li>S3</li><li>the EMR S3</li></ul>,S3A
<b> HBase works with which type of schema enforcement?</b><br><ul><li>schema on write</li><li>no schema</li><li>external schema</li><li>schema on read</li></ul>,schema on read
<b> HDFS file are of what type?</b><br><ul><li>read-write</li><li>read-only</li><li>write-only</li><li>append-only</li></ul>,append-only
<b> A distributed cache file path can originate from what location?</b><br><ul><li>hdfs or top</li><li>http</li><li>hdfs or http</li><li>hdfs</li></ul>,hdfs or http
<b> Which library should you use to perform ETL-type MapReduce jobs?</b><br><ul><li>Hive</li><li>Pig</li><li>Impala</li><li>Mahout</li></ul>,Pig
"<b> What is the output of the Reducer?</b><br><ul><li>a relational table</li><li>an update to the input file</li><li>a single, combined list</li><li>a set of <key, value> pairs`map function processes a certain key-value pair and emits a certain number of key-value pairs and the Reduce function processes values grouped by the same key and emits another set of key-value pairs as output.`<br></li></ul>","a set of <key, value> pairs`map function processes a certain key-value pair and emits a certain number of key-value pairs and the Reduce function processes values grouped by the same key and emits another set of key-value pairs as output.`<br>"
"<b> To optimize a Mapper, what should you perform first?</b><br><ul><li>Override the default Partitioner.</li><li>Skip bad records.</li><li>Break up Mappers that do more than one task into multiple Mappers.</li><li>Combine Mappers that do one task into large Mappers.</li></ul>",No answer found
"<b> When implemented on a public cloud, with what does Hadoop processing interact?</b><br><ul><li>files in object storage</li><li>graph data in graph databases</li><li>relational data in managed RDBMS systems</li><li>JSON data in NoSQL databases</li></ul>",files in object storage
"<b> In the Hadoop system, what administrative mode is used for maintenance?</b><br><ul><li>data mode</li><li>safe mode</li><li>single-user mode</li><li>pseudo-distributed mode</li></ul>",safe mode
"<b> In what format does RecordWriter write an output file?</b><br><ul><li><key, value> pairs</li><li>keys</li><li>values</li><li><value, key> pairs</li></ul>","<key, value> pairs"
<b> To what does the Mapper map input key/value pairs?</b><br><ul><li>an average of keys for values</li><li>a sum of keys for values</li><li>a set of intermediate key/value pairs</li><li>a set of final key/value pairs</li></ul>,a set of intermediate key/value pairs
<b> Which Hive query returns the first </b><br><ul><li>SELECT…WHERE value = 1000</li><li>SELECT … LIMIT 1000</li><li>SELECT TOP 1000 …</li><li>SELECT MAX 1000…</li></ul>,SELECT … LIMIT 1000
"<b> To implement high availability, how many instances of the master node should you configure?</b><br><ul><li>one</li><li>zero</li><li>shared</li><li>two or more (https://data-flair.training/blogs/hadoop-high-availability-tutorial)</li></ul>",two or more (https://data-flair.training/blogs/hadoop-high-availability-tutorial)
<b> Hadoop </b><br><ul><li>kubernetes</li><li>JobManager</li><li>JobTracker</li><li>YARN</li></ul>,YARN
"<b> In MapReduce, **\_** have \_</b><br><ul><li>tasks; jobs</li><li>jobs; activities</li><li>jobs; tasks</li><li>activities; tasks</li></ul>",jobs; tasks
<b> What type of software is Hadoop Common?</b><br><ul><li>database</li><li>distributed computing framework</li><li>operating system</li><li>productivity tool</li></ul>,distributed computing framework
"<b> If no reduction is desired, you should set the numbers of \_ tasks to zero</b><br><ul><li>combiner</li><li>reduce</li><li>mapper</li><li>intermediate</li></ul>",reduce
<b> MapReduce applications use which of these classes to report their statistics?</b><br><ul><li>mapper</li><li>reducer</li><li>combiner</li><li>counter</li></ul>,counter
"<b> \_ is the query language, and \_ is storage for NoSQL on Hadoop</b><br><ul><li>HDFS; HQL</li><li>HQL; HBase</li><li>HDFS; SQL</li><li>SQL; HBase</li></ul>",HQL; HBase
<b> MapReduce </b><br><ul><li>does not include</li><li>is the same thing as</li><li>includes</li><li>replaces</li></ul>,does not include
"<b> Which type of Hadoop node executes file system namespace operations like opening, closing, and renaming files and directories?</b><br><ul><li>ControllerNode</li><li>DataNode</li><li>MetadataNode</li><li>NameNode</li></ul>",NameNode
<b> HQL queries produce which job types?</b><br><ul><li>Impala</li><li>MapReduce</li><li>Spark</li><li>Pig</li></ul>,No answer found
<b> Suppose you are trying to finish a Pig script that converts text in the input string to uppercase. What code is needed on line </b><br><br>1 data = LOAD '/user/hue/pig/examples/data/midsummer.txt'...<br>2<br><br><ul><li>as (text:CHAR[]); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);</li><li>as (text:CHARARRAY); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);</li><li>as (text:CHAR[]); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);</li><li>as (text:CHARARRAY); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);</li></ul>,as (text:CHARARRAY); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);
"<b> In a MapReduce job, which phase runs after the Map phase completes?</b><br><ul><li>Combiner</li><li>Reducer</li><li>Map2</li><li>Shuffle and Sort</li></ul>",Combiner
<b> Where would you configure the size of a block in a Hadoop environment?</b><br><ul><li>dfs.block.size in hdfs-site.xmls</li><li>orc.write.variable.length.blocks in hive-default.xml</li><li>mapreduce.job.ubertask.maxbytes in mapred-site.xml</li><li>hdfs.block.size in hdfs-site.xml</li></ul>,dfs.block.size in hdfs-site.xmls
<b> Hadoop systems are **\_** RDBMS systems.</b><br><ul><li>replacements for</li><li>not used with</li><li>substitutes for</li><li>additions for</li></ul>,additions for
<b> Which object can be used to distribute jars or libraries for use in MapReduce tasks?</b><br><ul><li>distributed cache</li><li>library manager</li><li>lookup store</li><li>registry</li></ul>,distributed cache
"<b> To view the execution details of an Impala query plan, which function would you use ?</b><br><ul><li>explain</li><li>query action</li><li>detail</li><li>query plan</li></ul>",explain
<b> Which feature is used to roll back a corrupted HDFS instance to a previously known good point in time?</b><br><ul><li>partitioning</li><li>snapshot</li><li>replication</li><li>high availability</li></ul>,"snapshot<br><br>[Reference](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#:~:text=is%20not%20supported.-,Snapshots,known%20good%20point%20in%20time.)<br>"
<b> Hadoop Common is written in which language?</b><br><ul><li>C++</li><li>C</li><li>Haskell</li><li>Java</li></ul>,Java
<b> Which file system does Hadoop use for storage?</b><br><ul><li>NAS</li><li>FAT</li><li>HDFS</li><li>NFS</li></ul>,HDFS
<b> What kind of storage and processing does Hadoop support?</b><br><ul><li>encrypted</li><li>verified</li><li>distributed</li><li>remote</li></ul>,distributed
<b> Hadoop Common consists of which components?</b><br><ul><li>Spark and YARN</li><li>HDFS and MapReduce</li><li>HDFS and S3</li><li>Spark and MapReduce</li></ul>,No answer found
<b> Most Apache Hadoop committers' work is done at which commercial company?</b><br><ul><li>Cloudera</li><li>Microsoft</li><li>Google</li><li>Amazon</li></ul>,No answer found
"<b> To get information about Reducer job runs, which object should be added?</b><br><ul><li>Reporter</li><li>IntReadable</li><li>IntWritable</li><li>Writer</li></ul>",No answer found
"<b> After changing the default block size and restarting the cluster, to which data does the new size apply?</b><br><ul><li>all data</li><li>no data</li><li>existing data</li><li>new data</li></ul>",No answer found
"<b> Which statement should you add to improve the performance of the following query?</b><br><br>```<br>SELECT<br>c.id,<br>c.name,<br>c.email_preferences.categories.surveys<br>FROM customers c;<br>```<br><br><ul><li>GROUP BY</li><li>FILTER</li><li>SUB-SELECT</li><li>SORT</li></ul>",No answer found
<b> What custom object should you implement to reduce IO in MapReduce?</b><br><ul><li>Comparator</li><li>Mapper</li><li>Combiner</li><li>Reducer</li></ul>,No answer found
<b> You can optimize Hive queries using which method?</b><br><ul><li>secondary indices</li><li>summary statistics</li><li>column-based statistics</li><li>a primary key index</li></ul>,No answer found
"<b> If you are processing a single action on each input, what type of job should you create?</b><br><ul><li>partition-only</li><li>map-only</li><li>reduce-only</li><li>combine-only</li></ul>",No answer found
<b> The simplest possible MapReduce job optimization is to perform which of these actions?</b><br><ul><li>Add more master nodes.</li><li>Implement optimized InputSplits.</li><li>Add more DataNodes.</li><li>Implement a custom Mapper.</li></ul>,No answer found
"<b> When you implement a custom Writable, you must also define which of these object?</b><br><ul><li>a sort policy</li><li>a combiner policy</li><li>a compression policy</li><li>a filter policy</li></ul>",No answer found
